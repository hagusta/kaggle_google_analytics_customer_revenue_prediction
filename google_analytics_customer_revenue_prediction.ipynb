{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle, time, math\n",
    "from datetime import datetime\n",
    "import os \n",
    "import csv"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "to_drop=['visitStartTime','visitNumber','visitId','totals_hits','totals_pageviews','sessionId','fullVisitorId,'date']\n",
    "#['date_day', 'geoNetwork_networkDomain', 'totals_bounces', 'totals_bounces_imputed', 'totals_newVisits_imputed', 'totals_pageviews_imputed', 'trafficSource_adwordsClickInfo_adNetworkType', 'trafficSource_adwordsClickInfo_gclId', 'trafficSource_adwordsClickInfo_page', 'trafficSource_adwordsClickInfo_slot', 'trafficSource_keyword', 'trafficSource_referralPath']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\h_agu\\Desktop\\machine_learning\\Miniconda3\\envs\\tensorflow-gpu3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2785: DtypeWarning: Columns (12) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "#data=pd.read_csv('train_data.csv',index_col='index')\n",
    "data=pd.read_csv('train_data.csv')\n",
    "data['totals_hits_views_ratio']=data['totals_hits']/data['totals_pageviews']\n",
    "if 'totals_transactionRevenue' in data.columns:\n",
    "    y=data.pop('totals_transactionRevenue').to_frame()\n",
    "x=data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=x.drop(['visitStartTime','visitNumber','visitId','totals_hits','totals_pageviews','sessionId','fullVisitorId','date'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x=x.drop(['date_day', 'geoNetwork_networkDomain', 'totals_bounces', 'totals_bounces_imputed', 'totals_newVisits_imputed', 'totals_pageviews_imputed', 'trafficSource_adwordsClickInfo_adNetworkType', 'trafficSource_adwordsClickInfo_gclId', 'trafficSource_adwordsClickInfo_page', 'trafficSource_adwordsClickInfo_slot', 'trafficSource_keyword', 'trafficSource_referralPath'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in x.columns:\n",
    "    if 'visitStartTime_' in col:\n",
    "        x[col]=x[col].astype(np.object)\n",
    "    if 'date_' in col:\n",
    "        x[col]=x[col].astype(np.object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "features=x.columns.tolist()\n",
    "category_features=[]\n",
    "category_features_idx=[]\n",
    "for col in features:\n",
    "    if x[col].dtypes==np.object:\n",
    "        #print(col,np.object)\n",
    "        category_features.append(col)\n",
    "        category_features_idx.append(features.index(col))\n",
    "    if x[col].dtypes==np.bool:\n",
    "        #print(col,np.bool)\n",
    "        category_features.append(col)\n",
    "        category_features_idx.append(features.index(col))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier,Pool\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#xtrain,xtest,ytrain,ytest=train_test_split(x, y, test_size=0.1, random_state=42, shuffle=True, stratify=y)\n",
    "_y=y.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "_y.loc[(_y['totals_transactionRevenue']>0),'totals_transactionRevenue']=1\n",
    "_y.loc[(_y['totals_transactionRevenue']==0),'totals_transactionRevenue']=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "features=x.columns.tolist()\n",
    "category_features=[]\n",
    "category_features_idx=[]\n",
    "for col in features:\n",
    "    if x[col].dtype == np.object or x[col].dtype == np.bool:\n",
    "        category_features.append(col)\n",
    "        category_features_idx.append(features.index(col))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import fmin, tpe, Trials, STATUS_OK, STATUS_FAIL, hp, pyll"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## binary classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain,xtest,ytrain,ytest=train_test_split(x, _y, test_size=0.4, random_state=42, shuffle=True, stratify=_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sF=StratifiedKFold(n_splits=5,shuffle=True)\n",
    "cv_split=[]\n",
    "for train_idx,test_idx in sF.split(xtrain,ytrain):\n",
    "    pool_train=Pool(xtrain.iloc[train_idx],ytrain.iloc[train_idx],cat_features=category_features_idx)\n",
    "    pool_test=Pool(xtrain.iloc[test_idx],ytrain.iloc[test_idx],cat_features=category_features_idx)\n",
    "    cv_split.append((pool_train,pool_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_space = {\n",
    "            'depth': hp.choice('depth', [4,6]),\n",
    "            'border_count': hp.choice('border_count', [32,64,128]),\n",
    "            'learning_rate': hp.loguniform('learning_rate', -5, 0),\n",
    "            'random_strength': hp.choice('random_strength', [1, 5, 10, 20]),\n",
    "            'one_hot_max_size': hp.choice('one_hot_max_size', [5, 25, 225]),\n",
    "            'l2_leaf_reg': hp.loguniform('l2_leaf_reg', 0, np.log(10)),\n",
    "            'bagging_temperature': hp.uniform('bagging_temperature', 0, 1),\n",
    "            'leaf_estimation_iterations':hp.choice('leaf_estimation_iterations',[1,3,5,7,10]),\n",
    "            'max_ctr_complexity':hp.quniform('max_ctr_complexity',1,5,1),\n",
    "            'leaf_estimation_method':hp.choice('leaf_estimation_method',['Newton','Gradient']),\n",
    "            'rsm':hp.uniform('rsm',0,1),\n",
    "            'fold_len_multiplier':hp.choice('fold_len_multiplier',[2,3,4])\n",
    "            #'class_weights': (hp.choice('non_class_weights_ratio',[1]), hp.uniform('class_weights_ratio',1,20))\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(params=None, dtrain=None, dtest=None, n_estimators=None, seed=0, run_time=None, run_cv_id=0, eval_no=0, verbose=False):\n",
    "    global metric,column_names\n",
    "    #print(run_cv_id, eval_no)\n",
    "    path=\"./cv_run/\"+str(run_time)\n",
    "    if not os.path.isdir(path):\n",
    "        os.mkdir(path)\n",
    "    fpath=path+\"/\"+str(eval_no)+\".\"+str(run_cv_id)\n",
    "    if not os.path.isdir(fpath):\n",
    "        os.mkdir(fpath)    \n",
    "    params.update({\"iterations\": n_estimators})\n",
    "    params.update({\"eval_metric\": metric})\n",
    "    params.update({\"logging_level\": 'Silent'})\n",
    "    params.update({\"metric_period\": 100})\n",
    "    params.update({\"random_seed\": seed})\n",
    "    #params.update({\"leaf_estimation_method\": \"Newton\"})\n",
    "    #params.update({\"leaf_estimation_iterations\" : 10})\n",
    "    params.update({\"rsm\" : 1})\n",
    "    params.update({\"thread_count\" : 8})\n",
    "    params.update({\"fold_len_multiplier\": 2})\n",
    "    #params.update({\"max_ctr_complexity\":5})\n",
    "    params.update({\"train_dir\": fpath})\n",
    "    params.update({\"calc_feature_importance\" : True})\n",
    "    params.update({'od_type':'Iter'})\n",
    "    params.update({'od_wait':30})\n",
    "    \n",
    "    bst = CatBoostClassifier(**params)\n",
    "    bst.fit(dtrain, eval_set=dtest, use_best_model=True)\n",
    "    with open(fpath + \"/test_error.tsv\", \"r\") as f:\n",
    "        reader=np.array(list(csv.reader(f,delimiter='\\t'))).squeeze()\n",
    "    header=reader[0]\n",
    "    feature=dict()\n",
    "    for col, val in zip(features,bst.__dict__['_feature_importance']):\n",
    "        feature.update({col:val})\n",
    "    #pd.to_pickle(bst.__dict__['_feature_importance'],path+\"/feature_importance.\"+str(eval_no)+\".\"+str(run_cv_id))\n",
    "    pd.to_pickle(feature,path+\"/feature_importance.\"+str(eval_no)+\".\"+str(run_cv_id))\n",
    "    idx=(header==metric).argmax()\n",
    "    #print('idx',idx, metric)\n",
    "    results=(reader[1:reader.shape[0],idx]).astype(np.float)\n",
    "    \n",
    "    if metric=='AUC' or metric=='Accuracy':\n",
    "        #print(\"metric\",metric)\n",
    "        results=1-results\n",
    "    #print('results',results)\n",
    "    return bst, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_each_iter(results):\n",
    "    #global metric\n",
    "    lengs=[]\n",
    "    _=[lengs.append(len(res)) for res in results]\n",
    "    #if metric in ['AUC','Accuracy']:\n",
    "    mlen=np.max(lengs)\n",
    "    #else:\n",
    "    #    mlen=np.min(lengs)\n",
    "    #print(lengs,mlen)\n",
    "    a=[]\n",
    "    for run in results:\n",
    "        a.append((np.pad(run,(0,mlen-len(run)),'constant')).tolist())\n",
    "    x=np.array(a)\n",
    "    #print(x.shape)\n",
    "    means=[]\n",
    "    for i in range(x.shape[1]):\n",
    "        mean=0\n",
    "        count=0\n",
    "        for j in range(x.shape[0]):\n",
    "            if x[j,i] > 0:\n",
    "                count=count+1\n",
    "                mean=mean+x[j,i]\n",
    "        means.append(mean/count)\n",
    "    return means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_cv(cv_pairs, params=None, n_est=None, verbose=False, run_time=None):\n",
    "    global default_params,n_estimators,best_loss,hyperopt_eval_num,metric,hyperopt_evals,metric\n",
    "    params = params or default_params\n",
    "    n_estimators = n_est or n_estimators\n",
    "    #print('run_cv',hyperopt_eval_num)\n",
    "    evals_results, start_time = [], time.time()\n",
    "    _loss=[]\n",
    "    i=0\n",
    "    for dtrain, dtest in cv_pairs:\n",
    "        _, evals_result = fit(params, dtrain, dtest, n_estimators, run_time=run_time, run_cv_id=i, eval_no=hyperopt_eval_num+1)\n",
    "        #evals_results.append(np.mean(evals_result,axis=0))\n",
    "        evals_results.append(evals_result)\n",
    "        _loss.append(np.min(evals_result))\n",
    "        i=i+1\n",
    "    \n",
    "    mean_evals_results = mean_each_iter(evals_results)\n",
    "    best_n_estimators = np.argmin(mean_evals_results) + 1\n",
    "    eval_time = time.time() - start_time\n",
    "    \n",
    "    \n",
    "    cv_result = {'loss': mean_evals_results[best_n_estimators - 1] ,\n",
    "                 'best_n_estimators': best_n_estimators, \n",
    "                 'eval_time': eval_time,\n",
    "                 'status': STATUS_FAIL if np.isnan(mean_evals_results[best_n_estimators - 1]) else STATUS_OK,\n",
    "                 'params': params.copy(),\n",
    "                 'losses': _loss\n",
    "                }\n",
    "    best_loss = min(best_loss, cv_result['loss'])\n",
    "    hyperopt_eval_num += 1\n",
    "    cv_result.update({'hyperopt_eval_num': hyperopt_eval_num, 'best_loss': best_loss})\n",
    "        \n",
    "    if verbose:\n",
    "        print ('[{0}/{1}]\\teval_time={2:.2f} sec\\tcurrent_{3}={4:.6f}\\tmin_{3}={5:.6f}'.format(\n",
    "                    hyperopt_eval_num, hyperopt_evals, eval_time,\n",
    "                    metric, cv_result['loss'], best_loss))\n",
    "    return cv_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/20]\teval_time=82.28 sec\tcurrent_Accuracy=0.012743\tmin_Accuracy=0.012743\n",
      "[2/20]\teval_time=158.32 sec\tcurrent_Accuracy=0.011636\tmin_Accuracy=0.011636\n",
      "[3/20]\teval_time=87.10 sec\tcurrent_Accuracy=0.012743\tmin_Accuracy=0.011636\n",
      "[4/20]\teval_time=265.25 sec\tcurrent_Accuracy=0.012544\tmin_Accuracy=0.011636\n",
      "[5/20]\teval_time=706.57 sec\tcurrent_Accuracy=0.011324\tmin_Accuracy=0.011324\n",
      "[6/20]\teval_time=148.34 sec\tcurrent_Accuracy=0.012743\tmin_Accuracy=0.011324\n",
      "[7/20]\teval_time=329.07 sec\tcurrent_Accuracy=0.011689\tmin_Accuracy=0.011324\n",
      "[8/20]\teval_time=380.07 sec\tcurrent_Accuracy=0.012191\tmin_Accuracy=0.011324\n"
     ]
    }
   ],
   "source": [
    "n_estimators=1000\n",
    "max_evals = 20\n",
    "hyperopt_evals=max_evals\n",
    "metric=\"Accuracy\"\n",
    "\n",
    "this_trials = Trials()\n",
    "run_time=(datetime.now()).strftime('%Y%m%d%H%M')\n",
    "hyperopt_eval_num, best_loss, split_pair_data = 0, np.inf, None\n",
    "args=param_space\n",
    "_ = fmin(fn=lambda args: run_cv(cv_split, params=args, n_est=n_estimators, verbose=True,run_time=run_time), \n",
    "         space=args, algo=tpe.suggest, max_evals=max_evals, trials=this_trials)\n",
    "\n",
    "with open('./cv_run/'+run_time+'/trails.pickle','wb') as f:\n",
    "    pickle.dump(this_trials,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bagging_temperature': 0.08698568470862977,\n",
       " 'border_count': 32,\n",
       " 'depth': 4,\n",
       " 'fold_len_multiplier': 2,\n",
       " 'l2_leaf_reg': 6.511032253810222,\n",
       " 'leaf_estimation_iterations': 3,\n",
       " 'leaf_estimation_method': 'Gradient',\n",
       " 'learning_rate': 0.2774845333553973,\n",
       " 'max_ctr_complexity': 4.0,\n",
       " 'one_hot_max_size': 225,\n",
       " 'random_strength': 5,\n",
       " 'rsm': 1,\n",
       " 'iterations': 1000,\n",
       " 'eval_metric': 'Accuracy',\n",
       " 'logging_level': 'Silent',\n",
       " 'metric_period': 100,\n",
       " 'random_seed': 0,\n",
       " 'thread_count': 8,\n",
       " 'train_dir': './cv_run/201810100842/1.4',\n",
       " 'calc_feature_importance': True,\n",
       " 'od_type': 'Iter',\n",
       " 'od_wait': 30}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#this_trials.trials[0]['result']['params']\n",
    "#\n",
    "t=dict({'bagging_temperature': 0.08698568470862977,\n",
    " 'border_count': 32,\n",
    " 'depth': 4,\n",
    " 'fold_len_multiplier': 2,\n",
    " 'l2_leaf_reg': 6.511032253810222,\n",
    " 'leaf_estimation_iterations': 3,\n",
    " 'leaf_estimation_method': 'Gradient',\n",
    " 'learning_rate': 0.2774845333553973,\n",
    " 'max_ctr_complexity': 4.0,\n",
    " 'one_hot_max_size': 225,\n",
    " 'random_strength': 5,\n",
    " 'rsm': 1,\n",
    " 'iterations': 1000,\n",
    " 'eval_metric': 'Accuracy',\n",
    " 'logging_level': 'Silent',\n",
    " 'metric_period': 100,\n",
    " 'random_seed': 0,\n",
    " 'thread_count': 8,\n",
    " 'train_dir': './cv_run/201810100842/1.4',\n",
    " 'calc_feature_importance': True,\n",
    " 'od_type': 'Iter',\n",
    " 'od_wait': 30})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.011774447759999983\n",
      "0.01249559647999996\n",
      "0.01274274192\n",
      "0.01163612009999999\n",
      "0.011678540519999992\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'losses'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-49ea5f0852ea>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mthis_trials\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'result'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'losses'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m: 'losses'"
     ]
    }
   ],
   "source": [
    "for t in this_trials.trials:\n",
    "    print(np.mean(t['result']['losses']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('cv_run/201810091006/trails.pickle', 'rb') as f:\n",
    "    this_trials=pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.9875044\ttest: 0.9874427\tbest: 0.9874427 (0)\ttotal: 2.25s\tremaining: 1h 14m 54s\n",
      "Stopped by overfitting detector  (30 iterations wait)\n",
      "\n",
      "bestTest = 0.9881702641\n",
      "bestIteration = 67\n",
      "\n",
      "Shrink model to first 68 iterations.\n"
     ]
    }
   ],
   "source": [
    "metric='Accuracy'\n",
    "arg=this_trials.trials[1]['result']['params']\n",
    "run_time=(datetime.now()).strftime('%Y%m%d%H%M')\n",
    "trainP=Pool(xtrain,ytrain,cat_features=category_features_idx)\n",
    "testP=Pool(xtest,ytest,cat_features=category_features_idx)\n",
    "bst,res=fit(params=arg,dtrain=trainP,dtest=testP,n_estimators=2000,seed=13,run_time=run_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./model/model_class.param','wb') as f:\n",
    "    pickle.dump(this_trials.trials[1]['result']['params'],f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.9872573\ttest: 0.9872573\tbest: 0.9872573 (0)\ttotal: 981ms\tremaining: 32m 41s\n",
      "100:\tlearn: 0.9880891\ttest: 0.9877608\tbest: 0.9877857 (98)\ttotal: 1m 17s\tremaining: 24m 17s\n",
      "200:\tlearn: 0.9883952\ttest: 0.9880734\tbest: 0.9880734 (200)\ttotal: 2m 34s\tremaining: 22m 59s\n",
      "Stopped by overfitting detector  (30 iterations wait)\n",
      "\n",
      "bestTest = 0.988098334\n",
      "bestIteration = 211\n",
      "\n",
      "Shrink model to first 212 iterations.\n"
     ]
    }
   ],
   "source": [
    "metric='Accuracy'\n",
    "arg=this_trials.trials[14]['result']['params']\n",
    "run_time=(datetime.now()).strftime('%Y%m%d%H%M')\n",
    "#trainP=Pool(xtrain,np.log1p(ytrain),cat_features=category_features_idx)\n",
    "#testP=Pool(xtest,np.log1p(ytest),cat_features=category_features_idx)\n",
    "bst2,res=fit(params=arg,dtrain=trainP,dtest=testP,n_estimators=2000,seed=13,run_time=run_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.988170264094151 67\n"
     ]
    }
   ],
   "source": [
    "acc=bst.eval_metrics(testP,metrics=['Accuracy'])['Accuracy']\n",
    "print(np.max(acc),np.argmax(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "bst.save_model('./model/model_class.cbm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=CatBoostClassifier()\n",
    "loadbst=model.load_model('./model/model_class.cbm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols=xtrain.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.374778449831486 channelGrouping\n",
      "0.16139872908674058 date_dayofweek\n",
      "0.4619959745181547 date_dayofyear\n",
      "0.15348611827217473 date_month\n",
      "0.40288489033672004 date_quarter\n",
      "0.11906095008331209 date_year\n",
      "0.006196895048098561 device_browser\n",
      "0.8789421963215995 device_deviceCategory\n",
      "0.4571194588849443 device_isMobile\n",
      "0.5341052018264182 device_operatingSystem\n",
      "0.08648932507557155 geoNetwork_city\n",
      "0.0 geoNetwork_city_is_revenue_above_mean\n",
      "0.36070554382672576 geoNetwork_city_trans_view_above_1_per_mile\n",
      "5.361823010110651 geoNetwork_continent\n",
      "1.6723059099052713 geoNetwork_country\n",
      "1.639131962146519 geoNetwork_metro\n",
      "0.0 geoNetwork_metro_is_revenue_above_mean\n",
      "1.6411498941379035 geoNetwork_metro_trans_view_above_1_per_mile\n",
      "0.1481145762058296 geoNetwork_region\n",
      "2.4473920953268937 geoNetwork_subContinent\n",
      "2.0545450614988887 totals_hits_between_10_30\n",
      "3.3981187108614086 totals_hits_between_30_50\n",
      "3.5666578819293635 totals_hits_between_50_100\n",
      "0.7947971612713864 totals_hits_between_5_10\n",
      "1.0742484194723594 totals_hits_greater_100\n",
      "0.0 totals_hits_to_5\n",
      "4.337727671365749 totals_newVisits\n",
      "17.042459421615757 totals_pageviews_between_10_30\n",
      "6.185189576707915 totals_pageviews_between_30_50\n",
      "2.762933927024916 totals_pageviews_between_50_100\n",
      "9.304865036049806 totals_pageviews_between_5_10\n",
      "0.23228685014840142 totals_pageviews_greater_100\n",
      "16.489287491592073 totals_pageviews_to_5\n",
      "0.00013973827777518774 trafficSource_adContent\n",
      "0.07755857028676653 trafficSource_campaign\n",
      "0.19532188812774864 trafficSource_medium\n",
      "0.7869616709007339 trafficSource_source\n",
      "0.026872344683648998 visitStartTime_dayofmonth\n",
      "0.10587891025580044 visitStartTime_dayofweek\n",
      "0.0 visitStartTime_dayofyear\n",
      "0.9065967542036937 visitStartTime_hourofday\n",
      "0.2507596846522281 visitStartTime_month\n",
      "0.23574050851060913 visitStartTime_quarter\n",
      "0.4728239395563366 visitStartTime_year\n",
      "10.791147600061645 totals_hits_views_ratio\n",
      "['geoNetwork_city_is_revenue_above_mean', 'geoNetwork_metro_is_revenue_above_mean', 'totals_hits_to_5', 'visitStartTime_dayofyear']\n"
     ]
    }
   ],
   "source": [
    "zero_col=[]\n",
    "feat=bst.feature_importances_\n",
    "for c,f in zip(cols,bst.feature_importances_):\n",
    "    print(f,c)\n",
    "    if f==0:\n",
    "        zero_col.append(c)\n",
    "print(zero_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain,xtest,ytrain,ytest=train_test_split(x, y, test_size=0.4, random_state=42, shuffle=True, stratify=_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_t=_y.totals_transactionRevenue.loc[ytrain.index.to_series()]\n",
    "sF=StratifiedKFold(n_splits=5,shuffle=True)\n",
    "cv_split=[]\n",
    "for train_idx,test_idx in sF.split(xtrain,y_t):\n",
    "    pool_train_log=Pool(xtrain.iloc[train_idx],np.log1p(ytrain.iloc[train_idx]),cat_features=category_features_idx)\n",
    "    pool_test_log=Pool(xtrain.iloc[test_idx],np.log1p(ytrain.iloc[test_idx]),cat_features=category_features_idx)\n",
    "    cv_split.append((pool_train_log,pool_test_log))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "param=dict(\n",
    "{'bagging_temperature': 0.334998678341902,\n",
    " 'border_count': 128,\n",
    " 'depth': 4,\n",
    " 'l2_leaf_reg': 7.201514933380306,\n",
    " 'leaf_estimation_iterations': 5,\n",
    " 'leaf_estimation_method': 'Gradient',\n",
    " 'learning_rate': 0.8915171912524651,\n",
    " 'max_ctr_complexity': 4.0,\n",
    " 'one_hot_max_size': 225,\n",
    " 'random_strength': 1,\n",
    " 'iterations': 2001,\n",
    " \"logging_level\": 'Verbose',\n",
    " \"metric_period\": 100,\n",
    " #'eval_metric': 'Accuracy',\n",
    " 'logging_level': 'Silent',\n",
    " 'random_seed': 0,\n",
    " 'rsm': 1,\n",
    " 'thread_count': 8,\n",
    " 'fold_len_multiplier': 2,\n",
    " 'calc_feature_importance': True,\n",
    " 'od_type': 'Iter',\n",
    " 'od_wait': 30})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_space = {\n",
    "            'depth': hp.choice('depth', [4,6,8]),\n",
    "            'border_count': hp.choice('border_count', [32,64,128]),\n",
    "            'learning_rate': hp.loguniform('learning_rate', -5, 0),\n",
    "            'random_strength': hp.choice('random_strength', [1, 5, 10, 20]),\n",
    "            'one_hot_max_size': hp.choice('one_hot_max_size', [5, 25, 225]),\n",
    "            'l2_leaf_reg': hp.loguniform('l2_leaf_reg', 0, np.log(10)),\n",
    "            'bagging_temperature': hp.uniform('bagging_temperature', 0, 1),\n",
    "            'leaf_estimation_iterations':hp.choice('leaf_estimation_iterations',[1,3,5,7,10]),\n",
    "            'max_ctr_complexity':hp.quniform('max_ctr_complexity',1,5,1),\n",
    "            'leaf_estimation_method':hp.choice('leaf_estimation_method',['Newton','Gradient']),\n",
    "            'rsm':hp.uniform('rsm',0,1),\n",
    "            'fold_len_multiplier':hp.choice('fold_len_multiplier',[2,3,4])\n",
    "            #'class_weights': (hp.choice('non_class_weights_ratio',[1]), hp.uniform('class_weights_ratio',1,20))\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(params=None, dtrain=None, dtest=None, n_estimators=None, seed=0, run_time=None, run_cv_id=0, eval_no=0, verbose=False):\n",
    "    global metric,column_names\n",
    "    #print(run_cv_id, eval_no)\n",
    "    path=\"./cv_run/\"+str(run_time)\n",
    "    if not os.path.isdir(path):\n",
    "        os.mkdir(path)\n",
    "    fpath=path+\"/\"+str(eval_no)+\".\"+str(run_cv_id)\n",
    "    if not os.path.isdir(fpath):\n",
    "        os.mkdir(fpath)    \n",
    "    params.update({\"iterations\": n_estimators})\n",
    "    params.update({\"eval_metric\": metric})\n",
    "    params.update({\"logging_level\": 'Silent'})\n",
    "    params.update({\"metric_period\": 100})\n",
    "    params.update({\"random_seed\": seed})\n",
    "    #params.update({\"rsm\" : 1})\n",
    "    params.update({\"thread_count\" : 8})\n",
    "    #params.update({\"fold_len_multiplier\": 2})\n",
    "    params.update({\"train_dir\": fpath})\n",
    "    params.update({\"calc_feature_importance\" : True})\n",
    "    params.update({'od_type':'Iter'})\n",
    "    params.update({'od_wait':30})\n",
    "    bst = CatBoostRegressor(**params)\n",
    "    bst.fit(dtrain, eval_set=dtest, use_best_model=True)\n",
    "    with open(fpath + \"/test_error.tsv\", \"r\") as f:\n",
    "        reader=np.array(list(csv.reader(f,delimiter='\\t'))).squeeze()\n",
    "    header=reader[0]\n",
    "    feature=dict()\n",
    "    for col, val in zip(features,bst.__dict__['_feature_importance']):\n",
    "        feature.update({col:val})\n",
    "    #pd.to_pickle(bst.__dict__['_feature_importance'],path+\"/feature_importance.\"+str(eval_no)+\".\"+str(run_cv_id))\n",
    "    pd.to_pickle(feature,path+\"/feature_importance.\"+str(eval_no)+\".\"+str(run_cv_id))\n",
    "    idx=(header==metric).argmax()\n",
    "    #print('idx',idx, metric)\n",
    "    results=(reader[1:reader.shape[0],idx]).astype(np.float)\n",
    "    \n",
    "    if metric=='AUC' or metric=='Accuracy':\n",
    "        #print(\"metric\",metric)\n",
    "        results=1-results\n",
    "    #print('results',results)\n",
    "    return bst, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/2]\teval_time=18.84 sec\tcurrent_RMSE=1.812583\tmin_RMSE=1.812583\n",
      "[2/2]\teval_time=27.98 sec\tcurrent_RMSE=1.726956\tmin_RMSE=1.726956\n"
     ]
    }
   ],
   "source": [
    "n_estimators=10\n",
    "max_evals = 2\n",
    "hyperopt_evals=max_evals\n",
    "metric=\"RMSE\"\n",
    "\n",
    "this_trials = Trials()\n",
    "run_time=(datetime.now()).strftime('%Y%m%d%H%M')\n",
    "hyperopt_eval_num, best_loss, split_pair_data = 0, np.inf, None\n",
    "args=param_space\n",
    "_ = fmin(fn=lambda args: run_cv(cv_split, params=args, n_est=n_estimators, verbose=True,run_time=run_time), \n",
    "         space=args, algo=tpe.suggest, max_evals=max_evals, trials=this_trials)\n",
    "\n",
    "with open('./cv_run/'+run_time+'/trails.pickle','wb') as f:\n",
    "    pickle.dump(this_trials,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainP=Pool(xtrain,np.log1p(ytrain),cat_features=category_features_idx)\n",
    "testP=Pool(xtest,np.log1p(ytest),cat_features=category_features_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow-gpu3",
   "language": "python",
   "name": "tensorflow-gpu3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
